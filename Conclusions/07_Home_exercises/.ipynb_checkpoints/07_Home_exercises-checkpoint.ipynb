{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb489a09-529e-457d-93e8-b8bb5bf24e18",
   "metadata": {},
   "source": [
    "# 07 - Data wrangling\n",
    "\n",
    "This notebook contains solution proposals to the home exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae76bf8-cea2-4509-83ff-03ca2d4ac3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413ef3c-fe95-4344-8729-2fb62cb65a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdd7c015-1f2b-46e8-a23c-5fc6c57f2eb2",
   "metadata": {},
   "source": [
    "### ðŸ“š Exercise 1: Titanic aggregations\n",
    "\n",
    "Load and merge the data in <code>titanic.csv</code> and <code>titanic_additional.csv</code> to perform the following aggregations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f7bb7-a3c2-4c03-97ef-5175a8309626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "titanic_add = pd.read_csv('data/titanic_additional.csv')\n",
    "\n",
    "# Merge data \n",
    "titanic = titanic.merge(titanic_add, on = ['PassengerId', 'Name', 'Sex'], how = 'inner')\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03dee47-8a34-41f3-bee3-8931a604bd8e",
   "metadata": {},
   "source": [
    "1. Compute the average survival rate by sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a71c64-a300-4abf-80f7-9e344da0338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('Sex')['Survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d91b6e-05fb-4495-8903-1b06f6edd63d",
   "metadata": {},
   "source": [
    "2. Count the number of passengers aged +50. Compute the average survival rate by sex for this group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413877c-83ef-41ea-9d48-954ff5630ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(titanic[titanic['Age'] > 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f16e07-0057-4321-869a-3ffd45ad3a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[titanic['Age'] > 50].groupby('Sex')['Survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228752ad-9e2d-446e-bafe-fd45f78429f4",
   "metadata": {},
   "source": [
    "3. Count the number of passengers below the age of 20 by class and sex. Compute the average survival rate for this group by class and sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417a55b-882a-40b3-a1b0-fa2a2083431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(titanic[titanic['Age'] < 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d8c22-c9bd-4114-bbb0-7e29d477fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[titanic['Age'] < 20].groupby(['Sex', 'Pclass']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b35966-b34d-4b0c-b629-cc8ac332ea7b",
   "metadata": {},
   "source": [
    "4. Count the number of non-missing values in each column by class and sex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd600c7d-0d7b-494a-871f-7bf6fdc2c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(['Pclass', 'Sex']).count()  # Excl. missing observations\n",
    "# titanic.groupby(['Pclass', 'Sex']).size() # Incl. missing observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23829505-9cd1-450c-9450-a1af1b662909",
   "metadata": {},
   "source": [
    "5. Compute the minimum, maximum and average age by embarkation port (column `Embarked`) in a single `agg` operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b021ec-b5a1-413f-a0ae-4f3525d266d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 1\n",
    "titanic.groupby('Embarked')['Age'].agg(['min', 'max', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b0e73-a143-4625-89a6-032c37595ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 2\n",
    "titanic.groupby('Embarked').agg(\n",
    "    min_age = ('Age', 'min'),\n",
    "    max_age = ('Age', 'max'),\n",
    "    mean_age = ('Age', 'mean')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98076c6-1f19-453d-9160-ebda4e570edd",
   "metadata": {},
   "source": [
    "6. Compute the number of passengers, the average age and the fraction of women by embarkation port in a single `agg` operation.\n",
    "\n",
    "   *Hint*: to compute the fraction of women, you can first create a numerical indicator variable for females.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878e84e-1ded-41a4-a0c2-75b6292b2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Sex_ind'] = 0\n",
    "titanic.loc[titanic['Sex'] == 'female', 'Sex_ind'] = 1\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da41ddf-eb35-470f-aa2d-7af46d1d9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('Embarked').agg(\n",
    "    num_passengers = ('PassengerId', 'count'),\n",
    "    mean_age = ('Age', 'mean'),\n",
    "    share_women = ('Sex_ind', 'mean')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631487a4-d07b-4c80-b690-c22fcb0ca00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9263323-df6a-437e-85ee-ac0a5d82778b",
   "metadata": {},
   "source": [
    "### ðŸ“š Exercise 2: Working with string data\n",
    "\n",
    "In this exercise, you will work with the original Titanic data set in `titanic.csv` and additional data stored in `titanic_address.csv`, which contains the address for each passenger. Note that the second data set contains address information only for passengers from the UK, while all other passengers (non-UK) have missing address information.\n",
    "\n",
    "The goal of the exercise is to calculate the survival rate by country of residence (for this exercise, we restrict ourselves to the UK, so these will be England, Scotland, Wales etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40384412-f326-47dc-8ef4-153f951e8dde",
   "metadata": {},
   "source": [
    "**Task 1**: Load `titanic.csv` and `titanic_address.csv` into two DataFrames.\n",
    "\n",
    "Inspect the columns contained in both data sets. As you can see, the orignal data contains the full name including the title and potential maiden name (for married women) in a single column. The address data contains this information in seperate columns. You want to merge these data sets, but first you need to create common keys (i.e., columns) in both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a7b7e-5090-4cc1-b3d9-7ab41bb5500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bfc8a-7d8f-4bf0-ad1a-9ada4fbe5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_address = pd.read_csv('data/titanic_address.csv')\n",
    "\n",
    "titanic_address.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582f100-82b3-47ba-98ae-09c50c85f8d0",
   "metadata": {},
   "source": [
    "**Task 2**: In the DataFrame with the original Titanic data, split the name information into three columns just like the columns in the second DataFrame by doing the following:\n",
    "- Restrict the sample to men only. (This simplifies the task. Women in this data set have much more complicated names as they contain both their husband's and their maiden name). The filtered DataFrame should have 577 passengers.\n",
    "- Split the `Name` column by `,` to extract the last name and the remainder as seperate columns. You can achieve this by using the [`partition`]((https://pandas.pydata.org/docs/reference/api/pandas.Series.str.partition.html#pandas.Series.str.partition) string method) string method.\n",
    "- Split the remainder (containing the title and first name) using the space character `\" \"` as seperator to obtain individual columns for the title and the first name.\n",
    "- Store the three data series in the original DataFrame (using the column names `FirstName`,     `LastName` and `Title`) and delete the `Name` column which is no longer needed.\n",
    "\n",
    "*Hint*: Make sure that you don't have any leading or trailing whitespace at the start/end of the strings after partition. You can remove whitespace using the [`strip`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.strip.html) method\n",
    "```\n",
    "df['FirstName'].str.strip()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c474d63-fcd8-4540-a2d6-36862278d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only men\n",
    "titanic = titanic[titanic['Sex'] == 'male'].reset_index(drop = True).copy()\n",
    "\n",
    "print(len(titanic))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031120fb-066a-4aa5-a5b2-1775c05dd498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract last names and rest of names as pandas Series\n",
    "# (Note: use strip method to remove trailing whitespace at start and end of string)\n",
    "lastnames = titanic['Name'].str.partition(',')[0].str.strip()\n",
    "remainder = titanic['Name'].str.partition(',')[2].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad72fc-dab1-4e9f-a4cb-f4eb6684146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract titles and first names as pandas Series\n",
    "titles = remainder.str.partition(' ')[0].str.strip()\n",
    "firstnames = remainder.str.partition(' ')[2].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef161bd4-f988-4948-8d64-9adc6201c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop name column from df\n",
    "titanic.drop('Name', axis = 1, inplace = True)\n",
    "\n",
    "# Add new name columns to df\n",
    "titanic['Title'] = titles\n",
    "titanic['FirstName'] = firstnames\n",
    "titanic['LastName'] = lastnames\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42965c-620e-41ae-893c-c0bcc65b6e5b",
   "metadata": {},
   "source": [
    "**Task 3**: Merge the original Titanic data with the address data based on the name columns you just created using a *left join*. Since we don't have address information for non-UK residence, you can drop the passengers with missing addresses. The merged DataFrame should have 471 passengers with non-missing address information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15420bf-1e35-48b8-b80e-7149bb0f339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dfs\n",
    "titanic = titanic.merge(titanic_address, on = ['Title', 'FirstName', 'LastName'], how = 'left')\n",
    "\n",
    "# Drop rows with missing city info using dropna method\n",
    "titanic.dropna(subset = 'City', axis = 0, inplace = True)\n",
    "titanic.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# ...or alternatively, drop missings by filtering\n",
    "# titanic = titanic[titanic['City'].notna()].reset_index(drop = True).copy()\n",
    "\n",
    "print(len(titanic))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57049b8-ca32-4aed-83f9-e3e91c046eae",
   "metadata": {},
   "source": [
    "**Task 4**: The file `UK_post_codes.csv` contains UK post code prefixes (which you can ignore), the corresponding city, and the corresponding country.\n",
    "\n",
    "Import the file and merge this data with your passenger data set using a *left join*.\n",
    "\n",
    "*Hint*: The data with the post codes contains duplicate rows for countries due to many postal codes. Before merging, you should ensure that you have only one row for each country-city combination. You can drop duplicate rows using the [`drop_duplicates`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b927d6-79ff-4ce0-817f-7cbf170d3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import file\n",
    "df_post = pd.read_csv('data/UK_post_codes.csv')\n",
    "\n",
    "df_post.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781f1eb-e121-4462-852f-b791534fa515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated rows of country-city combinations\n",
    "df_post.drop_duplicates(subset = ['City', 'Country'], keep = 'first', inplace = True)\n",
    "\n",
    "# Drop postal code prefix\n",
    "df_post.drop('Prefix', axis = 1, inplace = True)\n",
    "\n",
    "print(len(df_post))\n",
    "print(df_post['City'].nunique())\n",
    "df_post.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16d38f-1318-4507-ab10-ed7f1ba0e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge titanic data and UK locations\n",
    "titanic = titanic.merge(df_post, on = 'City', how = 'left')\n",
    "\n",
    "print(len(titanic))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4db659-2cc4-45de-b12c-b1b1d8139b6f",
   "metadata": {},
   "source": [
    "**Task 5**: Using the finale DataFrame, compute the average survivial rate by country of residence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca6ff9-f565-45dc-978b-890fdc674b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('Country')['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b5dde-3e18-4791-9fec-44d7220cfdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1838d1fc-ed2d-478b-868d-7abff770ad0e",
   "metadata": {},
   "source": [
    "### ðŸ“š Exercise 3: Importing multiple files\n",
    "\n",
    "The subfolder `stocks` in the `data` folder contains data on prices and traded volume for each weekday in 2020 for 10 different companies. The data for each company is stored in a seperate csv file with the company ticker as the file name.\n",
    "\n",
    "Your task is to import and combine the data sets into a single DataFrame and then calculate the monthly sum of traded volume by company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf4dd1-b73d-46e2-ade9-50bf84542259",
   "metadata": {},
   "source": [
    "**Task 1**: Import the files and concat them into a single DataFrame. Make sure that the dates in the final DataFrame is a datetime object.\n",
    "\n",
    "*Hint*: Create a list of all the file names in the folder (e.g., use [`listdir`](https://docs.python.org/3/library/os.html#os.listdir) from `os` to generate the list) and then import each file in a for loop in which you append each DataFrame to a list. Use `concat` to combine all the DataFrames in the final list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3c70a-5449-49c2-b71a-a443397a1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store dfs in\n",
    "df_lst = []\n",
    "\n",
    "# Generate list of all files in \"stocks\" folder\n",
    "files = os.listdir('data/stocks')\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    # Import file\n",
    "    df_temp = pd.read_csv('data/stocks/' + file)\n",
    "\n",
    "    # Convert date to datetime\n",
    "    df_temp['Date'] = pd.to_datetime(df_temp['Date'])\n",
    "\n",
    "    # Add column with ticker name\n",
    "    df_temp['Ticker'] = file.split('.')[0] # split string on '.'\n",
    "    #df_temp['Ticker'] = files[0][:-4] # or index the string without last 4 characters\n",
    "\n",
    "    # Append temp df to list\n",
    "    df_lst.append(df_temp)\n",
    "\n",
    "print(len(df_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8f22a-f588-4bfd-aeac-673e90739a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat list of dfs into single df\n",
    "df_stocks = pd.concat(df_lst).reset_index(drop = True)\n",
    "\n",
    "print(len(df_stocks))\n",
    "print(df_stocks['Ticker'].unique())\n",
    "df_stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44326203-e850-41d4-89ea-d506f0705a0c",
   "metadata": {},
   "source": [
    "**Task 2**: Calculate the monthly sum of traded volume for each ticker in the data in three different ways:\n",
    "1. Compute the monthly sums \"manually\" by looping over the data instead of using pandas aggregation methods (e.g., `groupby`).\n",
    "\n",
    "   *Hint*: use a nested `for` loop, in which the outer loop iterates over ticker names, and the inner loop iterates over months. Recall that you can use the `dt` accessor to access time properties from a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f64a7-bca2-455e-a8d5-e1e0e0673eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = []\n",
    "months = []\n",
    "sums = []\n",
    "\n",
    "# Outer loop to filter on ticker\n",
    "for ticker in df_stocks['Ticker'].unique():\n",
    "    df1 = df_stocks[df_stocks['Ticker'] == ticker]\n",
    "\n",
    "    # Inner loop to filter on month\n",
    "    for month in range(1, 13):\n",
    "        df2 = df1[df1['Date'].dt.month == month]\n",
    "\n",
    "        # Append each ticker, month and sum to lists\n",
    "        tickers.append(ticker)\n",
    "        months.append(month)\n",
    "        sums.append(df2['Volume'].sum())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ae397-6783-4eac-9fe2-25b11e4a33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df from dict of lists from for loop\n",
    "pd.DataFrame({\n",
    "    'Ticker' : tickers,\n",
    "    'Month' : months,\n",
    "    'Volume' : sums\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06493fb-b9b3-4f99-b9d7-bddb19ecb223",
   "metadata": {},
   "source": [
    "2. Compute the monthly sums using the pandas aggregation method `groupby` instead of loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ebe02-4ada-4e3d-9c1a-31a6219d73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks.groupby([df_stocks['Date'].dt.month, 'Ticker'])['Volume'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec794017-8c36-40e6-b23e-65ea446f2e73",
   "metadata": {},
   "source": [
    "3. Compute the monthly sums also using the pandas method `resample`.\n",
    "\n",
    "**Note**: This works because our data contains only daily observations within a single year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653eea1-f38b-43b6-9cb0-efb69bc36743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks.set_index('Date').groupby('Ticker').resample('ME')['Volume'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64606e07-10d5-4dda-95b5-c6a6b86aac11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a2d0241-b759-49fd-9562-e3d4deb4f46d",
   "metadata": {},
   "source": [
    "### ðŸ“š Exercise 4: Reshaping electricity data\n",
    "\n",
    "The file `eurostat.xlsx` contains data on electricity consumption (in gigawatt-hours) for European countries from 2001 to 2023. \n",
    "\n",
    "1. Import the file and and keep only observations for the years 2001 to 2020 and for actual countries (i.e., drop the EU/Euro aggregates). The data should have 41 countries observed for 20 unique years.\n",
    "   \n",
    "   *Hint*: See the solution proposal to home exercise #2 in lecture 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c28799b-9d8a-4337-b194-afe039ca08cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import file\n",
    "df_euro = pd.read_excel(\n",
    "    'data/eurostat.xlsx', \n",
    "    sheet_name = 'Sheet 1',           # Specify which sheet to import\n",
    "    skiprows = list(range(9)) + [10], # Skip rows at the top of the file\n",
    "    skipfooter = 5,                   # Skip rows at the bottom of the file\n",
    "    na_values = ':'                   # Specify that \":\" indicates NaN\n",
    ")\n",
    "\n",
    "# Rename column to country\n",
    "df_euro.rename(columns = {'TIME' : 'Country'}, inplace = True)\n",
    "\n",
    "# Keep only columns with country and year observations\n",
    "# (keep only years 2000 to 2020)\n",
    "years = [str(i) for i in range(2001, 2021)]\n",
    "df_euro = df_euro[['Country'] + years].copy()\n",
    "\n",
    "# Drop aggregates\n",
    "df_euro = df_euro[~df_euro['Country'].isin(['European Union - 27 countries (from 2020)', 'Euro area â€“ 20 countries (from 2023)'])].copy()\n",
    "\n",
    "df_euro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e5cd1-dd57-4f31-86b5-fdfe800a50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "df_euro.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23241a14-f2da-41f4-8ef8-70e1612b9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique countries\n",
    "print(df_euro['Country'].nunique())\n",
    "print(df_euro['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f1f8e-eed7-4189-affa-acb3a0a10a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "df_euro.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c214b910-f640-4ddc-a360-bea935d3c4ee",
   "metadata": {},
   "source": [
    "2. Many countries have missing observations on electricity consumption in some year. Calculate how many years each country has a non-missing observation.\n",
    "\n",
    "   *Hint*: Reshape first the data from wide to long using the pandas method `pivot`, and then count the number of non-missing observations for each country in a `groupby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a605f-b4e3-4b21-a65a-0297d158353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data from long to wide\n",
    "df_euro = df_euro.melt(\n",
    "    id_vars = 'Country',\n",
    "    value_name = 'Electricity',\n",
    "    var_name =  'Year'\n",
    ")\n",
    "\n",
    "# Sort values according to year for each country\n",
    "df_euro.sort_values(['Country', 'Year'], inplace = True)\n",
    "\n",
    "df_euro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae4d91-1230-489e-9995-63a828c33b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many electricity observations for each country\n",
    "# (note: count method only includes non-missing values)\n",
    "country_year_counts = df_euro.groupby('Country')['Electricity'].count()\n",
    "\n",
    "country_year_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a347160-1abe-458e-b320-7f8804c0cc82",
   "metadata": {},
   "source": [
    "3. Drop the countries from the data that you do not observe for every single year between 2001 and 2020. Note that you should have 35 countries left in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d40ad4-384f-4fb4-893b-7d4b46e3818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we can filter the country-year counts that we created above\n",
    "max_years = df_euro['Year'].nunique()\n",
    "country_year_counts[country_year_counts == max_years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd8fc83-9750-44a0-a70e-1ab540561204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract countries (from index) in filtered country-year counts\n",
    "complete_countries = country_year_counts[country_year_counts == max_years].index\n",
    "\n",
    "# Filter df so that we only keep countries observed all years\n",
    "df_euro = df_euro[df_euro['Country'].isin(complete_countries)].copy()\n",
    "\n",
    "print(df_euro['Country'].nunique())\n",
    "df_euro.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e328fccd-1ede-4654-8a05-80c6d3c22211",
   "metadata": {},
   "source": [
    "4. Calculate the average annual electricity consumption for the countries with complete data. Display this in a horizontal bar plot that shows the countries in a descending order (high to low). Add a vertical line to the bar plot that shows the average annual electricity consumption across all the countries in the data (i.e., unweighted average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43669c1a-36b5-438b-966f-cda334f93139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create series with country averages using groupby (sort in ascending order)\n",
    "mean_series = df_euro.groupby('Country')['Electricity'].mean().sort_values()\n",
    "mean_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892f81d-0366-435a-8f65-4085bdd53e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('fivethirtyeight'):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "    # Create horizontal bar plot with country averages \n",
    "    ax.barh(\n",
    "        mean_series.index,  # Use series index (country names) on the xaxis\n",
    "        mean_series.values, # Use series values on the yaxis\n",
    "    )\n",
    "\n",
    "    # Add EU average as vertical line\n",
    "    ax.axvline(x = mean_series.mean(), color = 'red', lw = 2, label ='Overall Average')\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title('Average Annual Electricity Consumption (2000-2020)')\n",
    "    ax.set_xlabel('Annual consumption (in GWh)')\n",
    "    # (use StrMethodFormatter from ticker to add thousand seperator on xaxis)\n",
    "    ax.xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e897c3b-6d27-46b5-8d46-5c6775b34371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce049ab-5634-47c4-9fc5-d67715080bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
