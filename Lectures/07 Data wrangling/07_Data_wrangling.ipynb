{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8fdb2d2-a692-4c20-8a5b-2c413b41e6c5",
   "metadata": {},
   "source": [
    "# 07 - Data wrangling\n",
    "\n",
    "Data wrangling is the process of transforming data into a format the is more suitable for data analysis. This involves e.g., transforming variables, aggregating data, merging data sets etc. \n",
    "\n",
    "We have already seen how to perform simple operations on a DataFrame, e.g., creating new columns. This notebooks shows more advanced operations that are common in data wrangling:\n",
    "- Grouping data\n",
    "- Combining data\n",
    "- Reshaping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe3b95-5c0c-4558-b97e-7e8c09e72b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd30a70-ebe3-4617-9d16-d7032e4a6ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3208748f-4933-4dc2-b53a-966788325626",
   "metadata": {},
   "source": [
    "## Grouping data\n",
    "\n",
    "The pandas method `groupby` groups together rows based on the values in a single or multiple columns and returns an object that contains information about the groups.\n",
    "\n",
    "This is very helpful in data analysis as it helps us summarize information about different groups in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e255bfa-86ee-449d-ba7a-e9b9ed5cece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_dict = {\n",
    "    'Name'  : ['Ole', 'Jenny', 'Chang', 'Jonas'],\n",
    "    'Age' : [18, 19, 22, 20],\n",
    "    'Score' : [65.0, 58.0, 79.0, 95.0],\n",
    "    'Pass'  : ['yes', 'no', 'yes', 'yes']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(grade_dict)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ecdc7-f22b-4a3d-9473-653df01e605d",
   "metadata": {},
   "source": [
    "`groupby` returns an object that we can perform operations on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3906b-9eb7-47cb-a9f2-6b4177710303",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_group = df.groupby('Pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4068f0ea-49fc-4608-b5df-365ee29695e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbefb7-ed62-4289-a079-fc45b1cc69c0",
   "metadata": {},
   "source": [
    "#### Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ac2d5-08f7-407b-9e82-a8f00b019378",
   "metadata": {},
   "source": [
    "Often we want to apply an *aggregation* function on the data seperately for each group. By aggregation we mean that the result of a computation has a lower dimension than the original data.\n",
    "\n",
    "For example, we can use the `mean` function on a grouped object to calculate the average value in each numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d2d61b-ae2d-4e92-b1e7-f85dcf57c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_group.mean(numeric_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e55b6-ff2a-4df2-8ad6-28ad7c34f9ff",
   "metadata": {},
   "source": [
    "Note that groups and data aggregation also supports column indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83934972-20e9-4810-896f-c20860295efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_group['Score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb098a1-4744-4590-afe2-170c7bfb9220",
   "metadata": {},
   "source": [
    "We have already seen how we can use `value_counts` to count the number of passengers in our Titanic data that survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97712f40-5d15-4ac4-a725-e3a6156a28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "# Display value counts\n",
    "titanic['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f508b-b061-4521-a5c1-47668af8b574",
   "metadata": {},
   "source": [
    "But what if we want to know the number of passengers that survived in 1st, 2nd and 3rd class?\n",
    "\n",
    "Then we have to group the data together on the column `Pclass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f2e1e-dc40-421e-9912-835c983e9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('Pclass')['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e838f2-5271-49ae-8e2c-f2c8701ebaa1",
   "metadata": {},
   "source": [
    "Or we can use `mean` to calculate the average age of passengers traveling 1st, 2nd and 3rd class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15e5fd-cddc-4ee3-8808-4a0d833a9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('Pclass')['Age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87fac3c-17ca-4aff-acca-f144490f7eaf",
   "metadata": {},
   "source": [
    "But what if we want to know the average age for men and women traveling 1st, 2nd and 3rd class?\n",
    "\n",
    "We can group the data by *multiple* columns by passing a list of column names to `groupby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b71a96-2e50-4048-8f9d-d3d3ec096a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(['Pclass', 'Sex'])['Age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26b253-475f-4045-a4da-7018009f7991",
   "metadata": {},
   "source": [
    "There are numerous functions to aggregate grouped data, for example:\n",
    "- `mean`: compute average within each group\n",
    "- `sum`: sum values within each group\n",
    "- `std`, `var`: within-group standard deviation and variance\n",
    "- `median`: compute median within each group\n",
    "- `quantile`: compute quantiles within each group\n",
    "- `size`: number of observations in each group\n",
    "- `count`: number of non-missing observations in each group\n",
    "- `first`, `last`: first and last elements in each group\n",
    "- `min`, `max`: minimum and maximum elements within a group\n",
    "\n",
    "\n",
    "See the [official documentation](https://pandas.pydata.org/docs/user_guide/groupby.html#built-in-aggregation-methods) for a complete list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2024e3da-27b1-455a-b60d-6d3ba8d0ba19",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "    <p> Use the <TT>titanic</TT> data to find out what was the most expensive ticket, i.e. highest fare, in 1st, 2nd and 3rd class?\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4600650-571e-4048-9b89-b24d2ba4116c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fa7b9af-31af-4e31-8212-4405dd667b97",
   "metadata": {},
   "source": [
    "We can also perform aggregations on a column by applying the `agg` function on that column. Note that the name of the operation, e.g., mean, is now passed as a string in the function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13031355-552f-4866-8a54-2e74e4ea94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate group means in a slightly more complicated way\n",
    "titanic.groupby(['Pclass', 'Sex'])['Age'].agg('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa6f0bd-c742-47f6-b7a3-f94dcf4c6e9a",
   "metadata": {},
   "source": [
    "The benefit of using the `agg` function is that it allows us to perform multiple aggregations at the same time on a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ffd26-4f1a-4be6-8cc5-b5812facaca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(['Pclass', 'Sex'])['Age'].agg(['mean', 'median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d0d1fe-49fc-463d-acd2-52f586c1198f",
   "metadata": {},
   "source": [
    "Alternatively, we can use the slightly more advanced syntax to perform multiple aggregations on *multiple* columns in a grouped object:\n",
    "\n",
    "```\n",
    "groups.agg(\n",
    "    new_column_name1 = ('column_name1', 'operation1'),\n",
    "    new_column_name2 = ('column_name2', 'operation2')\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e7132-555e-4a49-8d8a-325d4d18332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(['Pclass', 'Sex']).agg(\n",
    "    average_age = ('Age', 'mean'),      # average age in group\n",
    "    max_fare = ('Fare', 'max')          # maximum fare in group\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e055d8-c0ba-462b-9913-d1a57bd16e13",
   "metadata": {},
   "source": [
    "**Plotting**\n",
    "\n",
    "Grouping data can also be very helpful in plotting. For example, let us plot the *share* of survivors by 1st, 2nd and 3rd class.\n",
    "\n",
    "First, we calculate the share of survivors in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f23ad-93d8-45b1-8a7c-8ee15cf26e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass = titanic.groupby('Pclass')['Survived'].mean()\n",
    "\n",
    "pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5613b40-14d0-4533-8114-ee24a4fa2b73",
   "metadata": {},
   "source": [
    "Second, we update the index of the `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97f429-c6c2-4f64-a7f5-b4fd6c5191e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass.index = ['1st class', '2nd class', '3rd class']\n",
    "\n",
    "pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b273d-5332-4a71-89d9-e55d7c598a99",
   "metadata": {},
   "source": [
    "Third, we use the `bar` function from `matplotlib` to show the share of survivors by class in a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef54a4b0-f3a8-4719-b40a-1cf67d1ea5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 3))\n",
    "\n",
    "ax.bar(pclass.index, pclass, width = 0.5)\n",
    "\n",
    "ax.set_ylabel('Share of survivors')\n",
    "ax.set_title('Survival rate on the Titanic (by class)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178862ed-bcec-4982-ba3e-c876fa8cfd38",
   "metadata": {},
   "source": [
    "#### Transformations\n",
    "\n",
    "In the previous section, we combined `groupby` with aggregation functions to reduce data on the group level to a single statistic, e.g., mean. Alternatively, we can combine `groupby` with the `transform` function to assign the result of a computation to a new column in the data. This will leave the number of observations unchanged (i.e., no aggregation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45161f3a-9543-4c0b-8a86-bb75d80c91fc",
   "metadata": {},
   "source": [
    "For example, we can create a new column that contains the average value of the fare for specific groups in the Titanic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58221ed-5388-4329-a0d0-cf2aa999beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average fare for each Pclass\n",
    "titanic['Fare_avg'] = titanic.groupby('Pclass')['Fare'].transform('mean')\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986238b1-3124-4ef8-a384-8da0294f7e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanic[(titanic['Pclass'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa692e6-3595-4d73-8b9f-292312b8cf5c",
   "metadata": {},
   "source": [
    "In general, we use `transform` instead of `agg` when we want to perform computation based on both the individual observations as well as some aggregate statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f1f50-9958-4705-b109-7d43da5ad36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between average fare (by Pclass) for each passenger\n",
    "titanic['Fare_diff'] = titanic['Fare'] - titanic['Fare_avg']\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a7fe8f-a04d-4d61-80cd-12e4e476a100",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "    <p> Compute the <em>excess</em> fare paid by each passenger relative to the minimum fare by sex and class, i.e., compute $Fare - min(Fare)$ by sex and class. \n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee8a7d-7233-4435-9e42-c62f6830dfbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5c12952-e28d-43de-8832-55b6ac486264",
   "metadata": {},
   "source": [
    "#### Time series data\n",
    "\n",
    "Data can also be grouped on time properties when the data is a time series.\n",
    "\n",
    "We have used the `to_datetime` function in pandas to convert timestamps (e.g., dates) from objects (strings) to the `datetime` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22406270-b744-49df-868f-85e3f5563f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = pd.read_csv('data/AAPL.csv')\n",
    "apple['Date'] = pd.to_datetime(apple['Date'])\n",
    "apple.sort_values('Date', inplace = True)\n",
    "\n",
    "apple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434e88e-9481-444d-a5a3-f13131fd262c",
   "metadata": {},
   "source": [
    "This can be very useful when working with time series. For example, it lets us easily filter the data on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921cb852-e065-49b3-9036-831f28449732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific date\n",
    "apple[apple['Date'] == '2020-01-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a243c1e-397d-4220-8bbd-01170c97583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on range of dates\n",
    "apple[(apple['Date'] >= '2020-03-15') & (apple['Date'] < '2020-06-15')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deacc20e-dacf-4f4c-86bd-5a2a5830e2c8",
   "metadata": {},
   "source": [
    "In addition, `datetime` objects have time-related properties that we can access with the `dt` accessor, e.g., `Year`, `Month`, `Day`, `Hour` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd0ee0-c48e-4047-be9c-44d49d01681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple['Month'] = apple['Date'].dt.month\n",
    "\n",
    "apple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175ee5b-15a7-415a-9812-8a5cb36df06a",
   "metadata": {},
   "source": [
    "We can use the values from the `dt` accessor to group time series data on time properties, and then aggregate or transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c98abd-c66f-4409-aed0-3fc5a1f14eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.groupby('Month')[['Open', 'Close']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689cbe5e-00c7-4ede-b461-140abc94b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.groupby('Month').agg(\n",
    "    Close_mean = ('Close', 'mean'),\n",
    "    Volume_sum = ('Volume', 'sum')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3570e-8ede-4f61-87c5-edcb4e459815",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.groupby('Month')['Close'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484f243-c530-4ff8-9300-cf2f0eebd8e4",
   "metadata": {},
   "source": [
    "Note that pandas offers several transformation functions that can be especially useful when working with time series.\n",
    "\n",
    "For example, we can use `diff` to compute the change between two adjacent observations (i.e., rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dff77a-9868-45b1-8a3f-c267c3e94ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple['Close_diff'] = apple['Close'].diff()\n",
    "\n",
    "apple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d3327-cf20-4e76-918a-f383ef07a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple['Volume_diff'] = apple.groupby('Month')['Volume'].diff()\n",
    "\n",
    "apple[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a9757-6db4-4eb1-b883-b29cf94abdbd",
   "metadata": {},
   "source": [
    "Other useful transformation functions are:\n",
    "- `ffill`: Forward fill NA values within each group\n",
    "- `bfill`: Back fill NA values within each group\n",
    "- `cumsum`: Compute the cumulative sum within each group\n",
    "- `pct_change`: Compute the percent change between adjacent values within each group\n",
    "- `shift`: Shift values up or down within each group\n",
    "\n",
    "See the [official documentation](https://pandas.pydata.org/docs/user_guide/groupby.html#built-in-transformation-methods) for a complete list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56434576-b2c0-431e-b71e-315f7b4aac15",
   "metadata": {},
   "source": [
    "Finally, note that pandas offer a special function called `resample` that we can use when we want to group observations by time period and apply some aggregation function. \n",
    "\n",
    "Note that `resample` offers an alternative to `groupby` when working with time series, but it requires that the index in the `DataFrame` is a `datetime` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee0e33-a0b0-4591-841f-4fea468f80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = pd.read_csv('data/AAPL.csv')\n",
    "apple['Date'] = pd.to_datetime(apple['Date'])\n",
    "apple.set_index('Date', inplace = True)\n",
    "\n",
    "apple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c076b4b4-d5e9-4d9c-bc04-15c388c0433a",
   "metadata": {},
   "source": [
    "We use `resample` by applying it on a `DataFrame` and pass it a string that describes how the observations should be grouped (`'YE'` for aggregation to years, `'QE'` for quarters, `'ME'` for months, `'W'` for weeks, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600963df-2995-48dd-8459-f535b4fa4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.resample('ME').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dcf026-de2c-4b06-840c-ae36099880b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.resample('W').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0849cc09-d1e8-4caa-b604-69045981dacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc8cb26c-4fbd-44a1-aa2f-0107bd9cd14d",
   "metadata": {},
   "source": [
    "## Combining data\n",
    "\n",
    "Pandas offers several different ways of combining multiple `DataFrames` along the row or column axes. The two most useful functions for combining data are `concat` and `merge`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98129790-19ab-48c3-af52-05c6455ca819",
   "metadata": {},
   "source": [
    "#### Concatenating \n",
    "\n",
    "We can use `concat` to *stack* `DataFrames` that share the same columns, but have different observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677935e-beff-4c72-94ff-e366514bc396",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_dict = {\n",
    "    'Name'  : ['Ole', 'Jenny', 'Chang', 'Jonas'],\n",
    "    'Age' : [18, 19, 22, 20],\n",
    "    'Score' : [65.0, 58.0, 79.0, 95.0],\n",
    "    'Pass'  : ['yes', 'no', 'yes', 'yes']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(grade_dict)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac402cd-248a-4a92-9705-d6a864efb753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dict with additional grades\n",
    "grade_dict2 = {\n",
    "    'Name'  : ['Nico', 'Maria', 'Mario', 'Janne'],\n",
    "    'Age'   : [18, 24, 21, 20], \n",
    "    'Score' : [67, 48, 92, 71], \n",
    "    'Pass'  : ['yes', 'no', 'yes', 'yes']\n",
    "}\n",
    "\n",
    "df2 = pd.DataFrame(grade_dict2)\n",
    "\n",
    "df2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa9025-d11d-4bc5-9c96-bc999cb2585f",
   "metadata": {},
   "source": [
    "As a default, `concat` stacks a list of `DataFrames` on top of each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33c98a-463d-4f78-9dda-d89237bf3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d440155-6397-4614-923b-9315249fd868",
   "metadata": {},
   "source": [
    "But what if the `DataFrames` do not have the exact same columns?\n",
    "\n",
    "Let us drop `Pass` from `df2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d5a41-b18a-4bf1-b198-9716b6a9d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop('Pass', axis = 1, inplace = True)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee09ba53-be4a-4721-9c57-e2271b7d7ab6",
   "metadata": {},
   "source": [
    "We can still concatenate the `DataFrames`. In that case, `concat` will simply fill the cells with missing data with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb86703-15bd-4586-bcdc-aa28d6564bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([df, df2]) \n",
    "\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fe8a1-84f6-4a30-a621-c30ab8f86358",
   "metadata": {},
   "source": [
    "However, note that `concat` also concatenates the index, which means that the index values are no longer unique for each observation (i.e. row). \n",
    "\n",
    "This can be fixed using the `reset_index` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc931d69-7538-40da-a1df-b3ed5cbe7643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.reset_index(inplace = True, drop = True)\n",
    "\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02486fe1-4f92-4c81-8cd1-ab7a25d12eef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "    <p> Load the data in the files <TT>FRED_monthly_1990.csv</TT> and <TT>FRED_monthly_2000.csv</TT> found in the <TT>data</TT> subfolder. The files contain macroeconomic time series for the 1990s and 2000s, respectively.\n",
    "\n",
    "- Concatenate the two data sets to get a final DataFrame with 240 observations.\n",
    "- Set the column <TT>DATE</TT> as the index in the newly created DataFrame.\n",
    "- Calculate the average unemplyment rate (column <TT>UNRATE</TT>) in the data by month.\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603038c2-d0c4-4284-803f-174b671b5a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d16e2cc6-9307-4601-9475-afb8bd630a56",
   "metadata": {},
   "source": [
    "The most common use of `concat` is when we have observations on the same variables scattered across multiple data sets. But it is also possible to concatenate data sets along the column dimension by specifying `axis = 1` in `concat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d147ec4-c8cd-42b4-9fc5-3ec1ea5ff141",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df, df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcafcc46-2bcf-4b18-939b-c04d733d8cb7",
   "metadata": {},
   "source": [
    "However, it is very rare that we want to \"stack\" data sets side-by-side in this way. Instead, we usually combine data along the column dimension by *merging* data sets according to one or several keys (i.e., common columns/identifiers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a6732c-8f4c-4614-900c-3ef01fddff07",
   "metadata": {},
   "source": [
    "See the [function documention](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) for more information on `concat`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146751b4-5d29-4d98-a2f0-57226f6791b7",
   "metadata": {},
   "source": [
    "#### Merging\n",
    "\n",
    "While concatenation simply appends (or stacks) blocks of rows or columns from multiple data sets, merging involves more control over how the data should be combined. \n",
    "\n",
    "We use `merge` to combine data sets that share the same observations (i.e., rows), but have different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35013a-c781-466d-94b0-d2b0bdbb025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "    'Name': ['Ole', 'Jenny', 'Chang', 'Jonas', 'Mario'], \n",
    "    'Score1' : [65.0, 58.0, 79.0, 95.0, 92.0]\n",
    "})\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab3e55-8eab-4950-852a-321925bfd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\n",
    "    'Name': ['Ole', 'Chang', 'Jonas', 'Mario', 'Nico', 'Maria'], \n",
    "    'Score2' : [70.0, 77.0, 92.0, 92.0, 72.0, 68.0]\n",
    "})\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba987a-cfba-4fe2-99d6-305e1b15514f",
   "metadata": {},
   "source": [
    "To merge two data sets, we apply the `merge` function on the first data set, and then pass the second data set as the first input to the function call. In addition, we need to specify the `on` parameter, which requires the label of the shared column between the two data sets (i.e., the key). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933c1470-4610-4a2d-873e-15e32727fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.merge(df2, on = 'Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae2c3f1-575f-488b-8270-35b2fa04eb16",
   "metadata": {},
   "source": [
    "Note that as a default, `merge` will combine only those observations (i.e., rows) found in both data sets. However, in data analysis, we often encounter the issue that some observations are present in one data set but not in the other. In that case, we also need to specify the `how` parameter in `merge`, which determines which subset of the data that we will retain in the final data set:\n",
    "\n",
    "1. `how='inner'` performs an *inner join*: the merged data contains only the intersection of keys that are present in both data sets.\n",
    "2. `how='outer'` performs an *outer join*: the merged data contains the union of keys present in either of the data sets.\n",
    "3. `how='left'` peforms a *left join*: all observations from the left data set are present in the final data, but rows that are only present in the right data are dropped.\n",
    "4. `how='right'` performs a *right join*: all observations from the right data set are present in the final data, but rows that are only present in the left data are dropped.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e43b72-29ba-4e8c-9519-f9ec10c85bdb",
   "metadata": {},
   "source": [
    "A left join keeps all the observations in the first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e862e-8de7-4330-a7f9-4974042e4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.merge(df2, on = 'Name', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c519bd0-6412-4bc5-b277-b1fa0c954b34",
   "metadata": {},
   "source": [
    "A right join keeps all the observations in the second data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b63b0-9fce-4f9d-9d4c-6daa69f00a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.merge(df2, on = 'Name', how = 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d5d93-afbf-42ec-89cf-171a512ece0b",
   "metadata": {},
   "source": [
    "An outer join keeps all observations in both data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ccfc2-b9f7-4c01-81a2-6b38dfc5657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.merge(df2, on = 'Name', how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea1b3c-0e71-41eb-81b2-f755e078e32e",
   "metadata": {},
   "source": [
    "Note that when the `DataFrames` have more than one common variable, we must merge on *multiple* keys. Otherwise, we get duplicate columns in the merged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b944fd7-e7a9-418e-8eb9-5081f5e2c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "    'Name'       : ['Ole', 'Jenny', 'Chang', 'Jonas', 'Mario'],\n",
    "    'Student_no' : ['s1001', 's1002', 's1003', 's1004', 's1005'],\n",
    "    'Score1'     : [65.0, 58.0, 79.0, 95.0, 92.0]\n",
    "})\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b54e1-c1b6-4c98-9f3e-512f46cda454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\n",
    "    'Name'       : ['Ole', 'Chang', 'Jonas', 'Mario', 'Nico', 'Maria'],\n",
    "    'Student_no' : ['s1001', 's1003', 's1004', 's1005', 's1006', 's1007'],\n",
    "    'Score2'     : [70.0, 77.0, 92.0, 92.0, 72.0, 68.0]\n",
    "})\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c253dbe-5a2c-45a3-8495-9806822a8947",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.merge(df2, on = 'Name', how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f873f9-9ec0-4fb8-8d85-9cadefcf26a0",
   "metadata": {},
   "source": [
    "We merge on multiple keys by passing a *list* of column labels to the `on` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90193112-0212-4925-95bd-1579e6bb4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df1.merge(df2, on = ['Name', 'Student_no'], how = 'outer')\n",
    "\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c573b-d764-4150-a02e-5a662d3702e4",
   "metadata": {},
   "source": [
    "See the [function documentation](https://pandas.pydata.org/docs/reference/api/pandas.merge.html) for more information on `merge`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe19bfe-f7f7-47c0-b1e5-7a8b2115fb2d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "    <p> The file <TT>titanic_additional.csv</TT> in the <TT>data</TT> subfolder contains additional information for the passengers on the Titanic: \n",
    "        \n",
    "- <TT>Ticket</TT>: ticket number\n",
    "- <TT>Cabin</TT>: Deck + cabin number\n",
    "- <TT>Embarked</TT>: Port at which passenger embarked: <TT>C</TT> - Cherbourg, <TT>Q</TT> - Queenstown, <TT>S</TT> - Southampton\n",
    "        \n",
    "Import the file and merge it with the <TT>titanic</TT> data.\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3714b-c492-40d7-a774-94f89dca343d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c81af-aeee-4692-beb0-da571c015517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d9c7be7-7a32-4836-a8fa-867ea0bb2651",
   "metadata": {},
   "source": [
    "## Reshaping data\n",
    "\n",
    "In data analysis, we often want *tidy* data, which is a standard format for organizing data sets do that variables, observations and values are consistently structured into columns, rows and cells:\n",
    "1. Each column is a variable\n",
    "2. Each row is an observation\n",
    "3. Each cell contains a single value\n",
    "\n",
    "<img src=\"images/tidy.png\" width = \"80%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd02cf-66b5-4695-b55c-de36a6869897",
   "metadata": {},
   "source": [
    "To ensure tidy data, we sometimes have to transform the shape of our data sets. In general, there are two types of data format: \n",
    "- **Long data**: Each row represents a single entity and different atributes of that entity are spread across multiple columns (fewer rows and more columns)\n",
    "- **Wide data**: Each row represents the value of a single attribute for a specific entity (more rows and fewer columns)\n",
    "\n",
    "<img src=\"images/format.png\" width = \"60%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64bd54b-6cab-42b1-9364-e70483905333",
   "metadata": {},
   "source": [
    "Let us create a wide data set in which we observe students and their score in different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ecd97-2a73-434b-bca2-1c83a30637b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df = pd.DataFrame({\n",
    "    'Student': ['Ole', 'Jenny', 'Chang', 'Jonas'],\n",
    "    'Math'   : [88, 92, 85, 79],\n",
    "    'English': [90, 85, 87, 93],\n",
    "    'PE'     : [95, 89, 92, 88]\n",
    "})\n",
    "\n",
    "wide_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105a6f7-06c5-4c0c-bb91-4440d26b9634",
   "metadata": {},
   "source": [
    "We can use `melt` to reshape the data from a wide to long format, i.e., a single column with the scores and a new column that indicates the subject.\n",
    "\n",
    "To use `melt`, we must pass a column label to the `id_vars` parameter. This is the column that denotes the entitites (i.e., the unit of observation) and which we want to leave \"untouched\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec70f4-d832-4cad-8d76-96a4be6a3c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df.melt(id_vars = 'Student')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653543cc-9671-4070-88a6-768acb505b10",
   "metadata": {},
   "source": [
    "In addition, we can pass arguments to the `var_name` and `value_name` parameters in order to specify the labels of the new columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d7006-250a-4612-ae58-7e5701e6f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = wide_df.melt(\n",
    "    id_vars = 'Student', \n",
    "    var_name = 'Subject', # Name of new column with old column labels\n",
    "    value_name = 'Score'  # Name of new column with old column values\n",
    ")\n",
    "\n",
    "long_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acbb65-b04a-4b09-89ad-55d703998b5f",
   "metadata": {},
   "source": [
    "See the [function documentation](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) for more information on `melt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0fb58-47c8-4ace-a3d1-1ece92c4bb03",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "    <p> Load the <TT>apple</TT> data and reshape the data from long to wide using only the price columns: \"Open\", \"High\", \"Low\" and \"Close\". The reshaped DataFrame should have 1,008 rows and the following columns:\n",
    "        \n",
    "- <TT>Date</TT>: Date of a given observation\n",
    "- <TT>Price</TT>: String indicating the type of price (e.g., \"Close\")\n",
    "- <TT>Value</TT>: Value of a given price metric on a given date\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1993bb-03ab-45cd-8a03-3f093211c63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08875b82-ecd1-4375-b8d0-7d1db9127a98",
   "metadata": {},
   "source": [
    "Reshaping data from long to wide is known as \"pivoting\". To pivot a <code>DataFrame</code>, we can use the function `pivot`.\n",
    "\n",
    "To use `pivot`, we must specify the following parameters:\n",
    "\n",
    "- `index`: column to use as the new index \n",
    "- `columns`: column to use as the new colum labels \n",
    "- `values`: column to use as the new values in the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247de97-4347-4d15-8a6e-60eb1bfd21d2",
   "metadata": {},
   "source": [
    "Let us use `pivot` to reshape our student data back to a wide format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116475a2-5bc6-4fb4-a759-64e26fcabdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.pivot(\n",
    "    index = 'Student',   # Column used as index in new df\n",
    "    columns = 'Subject', # Column used as new columns labels in the df\n",
    "    values = 'Score'     # Column used to populate the new columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd4e3e8-fc25-4455-be36-cb9bb532e184",
   "metadata": {},
   "source": [
    "As before, we can use `reset_index` to return the new index as a column back to the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5235e5-0b48-4b3e-9241-6784bb145791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot from long to wide (and reset index)\n",
    "wide_df = long_df.pivot(index = 'Student', columns = 'Subject', values = 'Score').reset_index()\n",
    "\n",
    "# Remove index name (not necessary, just to make it look nicer)\n",
    "# wide_df.rename_axis(None, axis = 1, inplace = True)\n",
    "\n",
    "wide_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ee6f0-ffd5-4df4-8abd-fb58b8cffd7d",
   "metadata": {},
   "source": [
    "See the [function documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html) for more information on `pivot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69deea5-64e5-4657-80cf-1cc6abe8c265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40d53e55-b5dc-42ed-87f1-3cc9fcc7c48a",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "- See the official [user guide](https://pandas.pydata.org/docs/user_guide/groupby.html) for more information and examples on how to group data in pandas.\n",
    "- See the official [user guide](https://pandas.pydata.org/docs/user_guide/merging.html) for more information and examples on how to combine data in pandas.\n",
    "- See the official [user guide](https://pandas.pydata.org/docs/user_guide/reshaping.html) for more information and examples on how to reshape data in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec175b23-c370-48a5-9e13-c205074defc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4769fb8-d0f2-4cb9-90bb-1f3dae7d3208",
   "metadata": {},
   "source": [
    "# Home exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadbe33e-f1d0-419c-bfba-1fdbc1954f7d",
   "metadata": {},
   "source": [
    "In the previous lecture, we saw how we could create non-value returning functions to display plots. Note that we can also create functions that returns a DataFrame. Using self-defined functions in data analysis is very useful, especially to automate the workflow such as applying the same operation on multiple data sets or columns in a DataFrame.\n",
    "\n",
    "However, we have to take care when designing functions that transform data as DataFrames are a *mutable* date type. In general, it is a good rule-of-thumb to make a copy of the original DataFrame inside a function to avoid the function altering the underlaying data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c93c7f-d3d5-45d1-b7da-c1713569d59f",
   "metadata": {},
   "source": [
    "For example, we can create a function that takes a DataFrame and converts all the column labels in the data to lowercase. We use the `copy` function to create a new copy of the data inside the function before performing the operation. The function returns a *copy* of the DataFrame, but with all column labels in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293054e5-f625-49fa-bf5d-f99503dba90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_cols(df):\n",
    "\n",
    "    col_names = []\n",
    "\n",
    "    # Loop over column labels and convert to lowercae\n",
    "    for col in df.columns:\n",
    "        col_name = col.lower()\n",
    "        col_names.append(col_name)\n",
    "\n",
    "    # Make a copy of old df\n",
    "    df_new = df.copy()      \n",
    "    \n",
    "    # Update column names in new df\n",
    "    df_new.columns = col_names\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bb9da-9758-461f-9d34-505dd3b7c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "apple = pd.read_csv('data/AAPL.csv')\n",
    "\n",
    "apple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb45c4-9b86-4068-9ccd-8c4d1c780e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column labels to lower case\n",
    "apple_new = lower_cols(apple)\n",
    "\n",
    "apple_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6d894-07b4-4c62-bd23-54d82f9170db",
   "metadata": {},
   "source": [
    "By working on a copy inside the function, the function call did not alter the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8295d30-1152-4e2b-87de-592175229143",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e129c9b-6a4a-47ca-9d4d-374a44588069",
   "metadata": {},
   "source": [
    "So far, we have focused on how to transform numeric data to suit the purpose of our analysis. However, most of the data that we deal with contain strings, i.e., text data (name, addresses, etc.). Often this data is not in the format needed for analysis, and we have to perform additional string manipulation to extract the data we need.\n",
    "\n",
    "Such string manipulation can be acheived using the pandas [string methods](https://pandas.pydata.org/docs/user_guide/text.html#string-methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5268c5b-45d0-47bb-993f-7bcff60021cc",
   "metadata": {},
   "source": [
    "These string method can be accessed using the `str` attribute of string columns.\n",
    "\n",
    "For example, let us use `lower` to convert all names in the Titanic data to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971d9a2-9424-4aed-ba6f-be00b3b4ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a654e-71bc-4988-8027-547ec49b8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Name'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc00566-e8a9-49b4-b7d8-110edacd3152",
   "metadata": {},
   "source": [
    "Or we can use the [`partition`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.partition.html) method to split a string column on the first space. This will return each part of the string as a seperate column in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ddb0e-864c-45ba-a844-ba7225557e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Name'].str.partition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc75ec-0cf8-454e-8851-187786018498",
   "metadata": {},
   "source": [
    "### ðŸ“š Exercise 1: Titanic aggregations\n",
    "\n",
    "Load and merge the data in <code>titanic.csv</code> and <code>titanic_additional.csv</code> to perform the following aggregations:\n",
    "1. Compute the average survival rate by sex.\n",
    "2. Count the number of passengers aged +50. Compute the average survival rate by sex for this group.\n",
    "3. Count the number of passengers below the age of 20 by class and sex. Compute the average survival rate for this group by class and sex.\n",
    "4. Count the number of non-missing values in each column by class and sex. \n",
    "5. Compute the minimum, maximum and average age by embarkation port (column `Embarked`) in a single `agg` operation. \n",
    "6. Compute the number of passengers, the average age and the fraction of women by embarkation port in a single `agg` operation.\n",
    "\n",
    "   *Hint*: to compute the fraction of women, you can first create a numerical indicator variable for females."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b604092-8d47-4177-8b93-7c3cb973edf9",
   "metadata": {},
   "source": [
    "### ðŸ“š Exercise 2: Working with Titanic string data\n",
    "\n",
    "In this exercise, you will work with the original Titanic data set in `titanic.csv` and additional data stored in `titanic_address.csv`, which contains the address for each passenger. Note that the second data set contains address information only for passengers from the UK, while all other passengers (non-UK) have missing address information.\n",
    "\n",
    "The goal of the exercise is to calculate the survival rate by country of residence (for this exercise, we restrict ourselves to the UK, so these will be England, Scotland, Wales etc.).\n",
    "\n",
    "**Task 1**: Load `titanic.csv` and `titanic_address.csv` into two DataFrames.\n",
    "\n",
    "Inspect the columns contained in both data sets. As you can see, the orignal data contains the full name including the title and potential maiden name (for married women) in a single column. The address data contains this information in seperate columns. You want to merge these data sets, but first you need to create common keys (i.e., columns) in both DataFrames.\n",
    "\n",
    "**Task 2**: In the DataFrame with the original Titanic data, split the name information into three columns just like the columns in the second DataFrame by doing the following:\n",
    "- Restrict the sample to men only. (This simplifies the task. Women in this data set have much more complicated names as they contain both their husband's and their maiden name). The filtered DataFrame should have 577 passengers.\n",
    "- Split the `Name` column by `,` to extract the last name and the remainder as seperate columns. You can achieve this by using the [`partition`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.partition.html#pandas.Series.str.partition) string method.\n",
    "- Split the remainder (containing the title and first name) using the space character `\" \"` as seperator to obtain individual columns for the title and the first name.\n",
    "- Store the three data series in the original DataFrame (using the column names `FirstName`,     `LastName` and `Title`) and delete the `Name` column which is no longer needed.\n",
    "\n",
    "*Hint*: Make sure that you don't have any leading or trailing whitespace at the start/end of the strings after partition. You can remove whitespace using the [`strip`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.strip.html) method\n",
    "```\n",
    "df['FirstName'].str.strip()\n",
    "```\n",
    "\n",
    "**Task 3**: Merge the original Titanic data with the address data based on the name columns you just created using a *left join*. Since we don't have address information for non-UK residence, you can drop the passengers with missing addresses. The merged DataFrame should have 471 passengers with non-missing address information.\n",
    "\n",
    "**Task 4**: The file `UK_post_codes.csv` contains UK post code prefixes (which you can ignore), the corresponding city, and the corresponding country.\n",
    "\n",
    "Import the file and merge this data with your passenger data set using a *left join*.\n",
    "\n",
    "*Hint*: The data with the post codes contains duplicate rows for countries due to the different postal codes. Before merging, you should ensure that you have only one row for each country-city combination. You can drop duplicate rows using the [`drop_duplicates`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) method.\n",
    "\n",
    "**Task 5**: Using the finale DataFrame, compute the average survivial rate by country of residence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8cc33-1e7e-4a8d-bd59-488f63289165",
   "metadata": {},
   "source": [
    "### ðŸ“š Exercise 3: Importing multiple stock files\n",
    "\n",
    "The subfolder `stocks` in the `data` folder contains data on prices and traded volume for each weekday in 2020 for 10 different companies. The data for each company is stored in a seperate csv file with the company ticker as the file name.\n",
    "\n",
    "Your task is to import and combine the data sets into a single DataFrame and then calculate the monthly sum of traded volume by company.\n",
    "\n",
    "**Task 1**: Import the files and combine them into a single DataFrame. Make sure that the dates in the final DataFrame is a datetime object.\n",
    "\n",
    "*Hint*: Create a list of all the file names in the folder (e.g., use [`listdir`](https://docs.python.org/3/library/os.html#os.listdir) from `os` to generate the list) and then import each file in a `for` loop in which you append each DataFrame to a list. Use `concat` to combine all the DataFrames in the final list.\n",
    "\n",
    "**Task 2**: Calculate the monthly sum of traded volume for each ticker in the data in three different ways:\n",
    "1. Compute the monthly sums \"manually\" by looping over the data instead of using pandas aggregation methods (e.g., `groupby`).\n",
    "\n",
    "   *Hint*: use a nested `for` loop, in which the outer loop iterates over ticker names, and the inner loop iterates over months. Recall that you can use the `dt` accessor to access time properties from a datetime object.\n",
    "2. Compute the monthly sums using the pandas aggregation method `groupby` instead of loops.\n",
    "3. Compute the monthly sums also using the pandas method `resample`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5830f2-2003-4ebf-910b-9ad267cfdd05",
   "metadata": {},
   "source": [
    "### ðŸ“š Exercise 4: Reshaping electricity data\n",
    "\n",
    "The file `eurostat.xlsx` contains data on electricity consumption (in gigawatt-hours) for European countries from 2001 to 2023. \n",
    "\n",
    "1. Import the file and and keep only observations for the years 2001 to 2020 and for actual countries (i.e., drop the EU/Euro aggregates). The data should have 41 countries observed for 20 unique years.\n",
    "   \n",
    "   *Hint*: See the solution proposal to home exercise #2 in lecture 5.\n",
    "   \n",
    "2. Many countries have missing observations on electricity consumption in some year. Calculate how many years each country has a non-missing observation.\n",
    "\n",
    "   *Hint*: Reshape first the data from wide to long using the pandas method `pivot`, and then count the number of non-missing observations for each country in a `groupby`.\n",
    "\n",
    "3. Drop the countries from the data that you do not observe for every single year between 2001 and 2020. Note that you should have 35 countries left in the data.\n",
    "\n",
    "4. Calculate the average annual electricity consumption for the countries with complete data. Display this in a horizontal bar plot that shows the countries in a descending order (high to low). Add a vertical line to the bar plot that shows the average annual electricity consumption across all the countries in the data (i.e., unweighted average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830cf338-c9ad-4c6d-a959-6bc4096dd4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bb780-e7c7-41ad-8b9a-19fd22a6f30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
