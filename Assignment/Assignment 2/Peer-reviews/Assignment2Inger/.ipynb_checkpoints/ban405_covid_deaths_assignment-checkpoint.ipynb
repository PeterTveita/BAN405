{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837ba798",
   "metadata": {},
   "source": [
    "# Wrangle and Visualize Global COVID-19 Deaths\n",
    "**Course:** BAN405 – Python Programming  \n",
    "**File created:** 2025-10-14\n",
    "\n",
    "This notebook follows the assignment instructions to wrangle and visualize COVID-19 deaths using the Johns Hopkins CSSE time series dataset and a country–continent lookup.\n",
    "\n",
    "> **How to run:**  \n",
    "> 1) Place the CSV files in a local folder named `data/` (next to this notebook):  \n",
    "> &nbsp;&nbsp;• `data/time_series_covid19_deaths_global.csv` (JHU CSSE)  \n",
    "> &nbsp;&nbsp;• `data/Countries Continents.csv` (country→continent mapping from OWID)  \n",
    "> 2) Then run the notebook top-to-bottom (Kernel → Restart & Run All).\n",
    ">\n",
    "> If the files are missing locally, the notebook **attempts** to fetch them from public GitHub URLs as a fallback. If your environment has no internet, place the files manually in `data/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55350f61",
   "metadata": {},
   "source": [
    "> **Statement on use of generative AI**  \n",
    "> I used ChatGPT (GPT‑5 Thinking) to help draft and organize this notebook, including code structure, comments, and markdown explanations. I reviewed, edited, and verified the content, and I am responsible for the final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff70cac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (4045239516.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    JHU_DEATHS_URL = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/\" \\                 \"csse_covid_19_data/csse_covid_19_time_series/\" \\                 \"time_series_covid19_deaths_global.csv\"\u001b[0m\n\u001b[0m                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot style (keep default; do not set specific colors per course rules)\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "PLOTS_DIR = Path(\"plots\")\n",
    "PLOTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# URLs (fallback if local files are missing)\n",
    "JHU_DEATHS_URL = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/\" \\                 \"csse_covid_19_data/csse_covid_19_time_series/\" \\                 \"time_series_covid19_deaths_global.csv\"\n",
    "\n",
    "# A commonly used OWID continents mapping (structure: country, continent)\n",
    "# Your local file may be named 'Countries Continents.csv' with similar columns.\n",
    "OWID_CONTINENTS_URL = \"https://raw.githubusercontent.com/owid/covid-19-data/master/scripts/input/continents/continents.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da9ad0",
   "metadata": {},
   "source": [
    "## Task 1 – Data wrangling\n",
    "### 1. Load and explore the COVID data set\n",
    "We load the JHU deaths time series. The dataset contains cumulative deaths by Province/State and Country/Region with date columns in `m/d/yy` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: load CSV from local 'data/' if present; otherwise try GitHub (internet required)\n",
    "def load_csv(local_name: str, fallback_url: str) -> pd.DataFrame:\n",
    "    local_path = DATA_DIR / local_name\n",
    "    if local_path.exists():\n",
    "        df = pd.read_csv(local_path)\n",
    "        print(f\"Loaded local file: {local_path}\")\n",
    "        return df\n",
    "    else:\n",
    "        try:\n",
    "            df = pd.read_csv(fallback_url)\n",
    "            print(f\"Loaded from web fallback: {fallback_url}\")\n",
    "            # Save a local copy so subsequent runs work offline\n",
    "            DATA_DIR.mkdir(exist_ok=True)\n",
    "            df.to_csv(local_path, index=False)\n",
    "            print(f\"Saved a local copy to: {local_path}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Could not find {local_path} and failed to download from the web.\n",
    "\"\n",
    "                f\"Error: {e}\\nPlease place the file in the 'data/' folder and rerun.\"\n",
    "            )\n",
    "\n",
    "# Load datasets\n",
    "deaths_raw = load_csv(\"time_series_covid19_deaths_global.csv\", JHU_DEATHS_URL)\n",
    "\n",
    "print(\"Shape:\", deaths_raw.shape)\n",
    "display(deaths_raw.head())\n",
    "display(deaths_raw.tail())\n",
    "print(\"\\nColumns:\", list(deaths_raw.columns)[:10], \"...\")\n",
    "print(deaths_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb42b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic exploration\n",
    "countries_unique = deaths_raw[\"Country/Region\"].nunique()\n",
    "provinces_present = deaths_raw[\"Province/State\"].notna().sum()\n",
    "print(f\"Unique countries/regions: {countries_unique}\")\n",
    "print(f\"Rows with a province/state value: {provinces_present}\")\n",
    "\n",
    "# Identify date columns programmatically (JHU file: first 4 are meta columns)\n",
    "meta_cols = [\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"]\n",
    "date_cols = [c for c in deaths_raw.columns if c not in meta_cols]\n",
    "print(f\"Detected {len(date_cols)} date columns. First & last:\", date_cols[0], \"…\", date_cols[-1])\n",
    "\n",
    "# Check missing values in key columns\n",
    "print(\"\\nMissing values per meta column:\")\n",
    "print(deaths_raw[meta_cols].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7980c",
   "metadata": {},
   "source": [
    "### 2. Reshape from wide to long (tidy)\n",
    "We use `pandas.melt()` to pivot date columns into a single `date` column for easier time-series handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_long = deaths_raw.melt(\n",
    "    id_vars=meta_cols,\n",
    "    value_vars=date_cols,\n",
    "    var_name=\"date_str\",\n",
    "    value_name=\"total_deaths\"\n",
    ")\n",
    "\n",
    "# Convert date strings to datetime\n",
    "deaths_long[\"date\"] = pd.to_datetime(deaths_long[\"date_str\"], format=\"%m/%d/%y\")\n",
    "\n",
    "# Keep relevant columns and ensure numeric\n",
    "deaths_long[\"total_deaths\"] = pd.to_numeric(deaths_long[\"total_deaths\"], errors=\"coerce\").fillna(0).astype(\"Int64\")\n",
    "deaths_long = deaths_long.drop(columns=[\"date_str\"])\n",
    "\n",
    "display(deaths_long.head())\n",
    "print(deaths_long.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353e7b1",
   "metadata": {},
   "source": [
    "### 3–4. Convert dates to timestamps and aggregate to the country level\n",
    "We sum across provinces/states within each country for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_daily = (\n",
    "    deaths_long\n",
    "    .groupby([\"Country/Region\", \"date\"], as_index=False)[\"total_deaths\"]\n",
    "    .sum()\n",
    "    .sort_values([\"Country/Region\", \"date\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Aggregated shape:\", country_daily.shape)\n",
    "display(country_daily.head())\n",
    "display(country_daily.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b4df8",
   "metadata": {},
   "source": [
    "### 5. Compute daily new deaths\n",
    "Daily new deaths are the difference in cumulative totals between two adjacent days. We use `groupby().diff()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_daily[\"new_deaths\"] = (\n",
    "    country_daily\n",
    "    .groupby(\"Country/Region\")[\"total_deaths\"]\n",
    "    .diff()\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Some countries can have occasional data corrections (negative diffs). Clip to zero for plotting clarity.\n",
    "country_daily[\"new_deaths\"] = country_daily[\"new_deaths\"].clip(lower=0)\n",
    "\n",
    "display(country_daily.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d9c38",
   "metadata": {},
   "source": [
    "## Task 2 – Data visualization\n",
    "### 1a) Total deaths over time for the **three countries with the highest totals** (as of the last date)\n",
    "We compute the top 3 countries by total deaths at the latest date in the data and plot their cumulative trajectories on one chart. The plot is saved as `plots/total_deaths.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_date = country_daily[\"date\"].max()\n",
    "latest_totals = (\n",
    "    country_daily[country_daily[\"date\"] == latest_date]\n",
    "    .sort_values(\"total_deaths\", ascending=False)\n",
    "    .head(3)\n",
    ")\n",
    "\n",
    "top3_countries = latest_totals[\"Country/Region\"].tolist()\n",
    "print(\"Latest date:\", latest_date.date())\n",
    "print(\"Top 3 countries:\", top3_countries)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "for country in top3_countries:\n",
    "    subset = country_daily[country_daily[\"Country/Region\"] == country]\n",
    "    plt.plot(subset[\"date\"], subset[\"total_deaths\"], label=country)\n",
    "\n",
    "plt.title(\"Total COVID-19 Deaths Over Time (Top 3 Countries)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total deaths (cumulative)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = PLOTS_DIR / \"total_deaths.png\"\n",
    "plt.savefig(out_path)\n",
    "plt.show()\n",
    "print(f\"Saved: {out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8189d1",
   "metadata": {},
   "source": [
    "### 1b) Daily new deaths for **Norway, Denmark, and Sweden** as three subplots\n",
    "Saved as `plots/new_deaths.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nordics = [\"Norway\", \"Denmark\", \"Sweden\"]\n",
    "fig, axes = plt.subplots(3, 1, figsize=(9, 8), sharex=True)\n",
    "\n",
    "for ax, country in zip(axes, nordics):\n",
    "    subset = country_daily[country_daily[\"Country/Region\"] == country]\n",
    "    ax.plot(subset[\"date\"], subset[\"new_deaths\"])\n",
    "    ax.set_title(country)\n",
    "    ax.set_ylabel(\"New deaths\")\n",
    "\n",
    "axes[-1].set_xlabel(\"Date\")\n",
    "fig.suptitle(\"Daily New COVID-19 Deaths – Nordics\", y=0.95)\n",
    "fig.tight_layout()\n",
    "\n",
    "out_path = PLOTS_DIR / \"new_deaths.png\"\n",
    "fig.savefig(out_path)\n",
    "plt.show()\n",
    "print(f\"Saved: {out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc07a4",
   "metadata": {},
   "source": [
    "### OPTIONAL: Reusable plotting function\n",
    "`plot_total_deaths(countries, data)` plots cumulative deaths for any selection of countries on the same axes with basic input validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_total_deaths(countries, data):\n",
    "    \"\"\"\n",
    "    Plot cumulative deaths over time for one or more countries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    countries : list[str]\n",
    "        Country names as they appear in `data['Country/Region']`.\n",
    "    data : pd.DataFrame\n",
    "        DataFrame with columns ['Country/Region', 'date', 'total_deaths'].\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not isinstance(countries, (list, tuple)):\n",
    "        raise TypeError(\"`countries` must be a list or tuple of country names.\")\n",
    "    if len(countries) == 0:\n",
    "        print(\"No countries provided. Nothing to plot.\")\n",
    "        return None\n",
    "    if not set([\"Country/Region\", \"date\", \"total_deaths\"]).issubset(data.columns):\n",
    "        raise ValueError(\"`data` must contain 'Country/Region', 'date', and 'total_deaths'.\")\n",
    "\n",
    "    known = set(data[\"Country/Region\"].unique())\n",
    "    requested = set(countries)\n",
    "    unknown = sorted(list(requested - known))\n",
    "    valid = [c for c in countries if c in known]\n",
    "\n",
    "    if unknown:\n",
    "        print(f\"Warning: Unknown countries ignored: {unknown}\")\n",
    "    if not valid:\n",
    "        print(\"No valid countries to plot. Nothing to do.\")\n",
    "        return None\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for c in valid:\n",
    "        subset = data[data[\"Country/Region\"] == c]\n",
    "        plt.plot(subset[\"date\"], subset[\"total_deaths\"], label=c)\n",
    "    plt.title(\"Total COVID-19 Deaths\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Total deaths (cumulative)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Quick tests\n",
    "tests = [\n",
    "    ['Norway', 'Denmark', 'Sweden'],\n",
    "    ['Norway', 'Denmark', 'Atlantis'],\n",
    "    ['The moon', 'Mars', 'Atlantis'],\n",
    "    []\n",
    "]\n",
    "for t in tests:\n",
    "    print(\"\\nTest:\", t)\n",
    "    plot_total_deaths(t, country_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec1da9",
   "metadata": {},
   "source": [
    "## Task 3 – Data merging with continents\n",
    "We merge a country→continent mapping onto our country-day data. Minor name harmonization improves the match (e.g., `US` vs `United States`, `Korea, South` vs `South Korea`). After merging we (1) check coverage, (2) compute **total deaths per continent**, and (3) make bar charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54014add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load continents mapping\n",
    "def load_continents():\n",
    "    # The assignment mentions a local file named 'Countries Continents.csv'.\n",
    "    # We'll try that first, then fall back to an OWID mapping hosted on GitHub.\n",
    "    try:\n",
    "        cc = pd.read_csv(DATA_DIR / \"Countries Continents.csv\")\n",
    "        print(\"Loaded local 'Countries Continents.csv'\")\n",
    "    except Exception:\n",
    "        cc = pd.read_csv(OWID_CONTINENTS_URL)\n",
    "        print(\"Loaded continents from OWID fallback URL\")\n",
    "        # Normalize likely column names to expected ones\n",
    "    # Try to coerce to common column names\n",
    "    lower_cols = {c.lower(): c for c in cc.columns}\n",
    "    # Heuristics for column naming\n",
    "    country_col = None\n",
    "    for cand in [\"country\", \"location\", \"entity\", \"name\", \"Country\", \"Location\"]:\n",
    "        if cand.lower() in lower_cols:\n",
    "            country_col = lower_cols[cand.lower()]\n",
    "            break\n",
    "    continent_col = None\n",
    "    for cand in [\"continent\", \"Continent\"]:\n",
    "        if cand.lower() in lower_cols:\n",
    "            continent_col = lower_cols[cand.lower()]\n",
    "            break\n",
    "    if country_col is None or continent_col is None:\n",
    "        raise ValueError(\"Could not infer country/continent columns from the continents file.\")\n",
    "\n",
    "    cc = cc.rename(columns={country_col: \"country\", continent_col: \"continent\"})\n",
    "    cc[\"country\"] = cc[\"country\"].astype(str)\n",
    "    cc[\"continent\"] = cc[\"continent\"].astype(str)\n",
    "    return cc[[\"country\", \"continent\"]]\n",
    "\n",
    "continents = load_continents()\n",
    "display(continents.head())\n",
    "print(\"Unique continents:\", sorted(continents[\"continent\"].dropna().unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name harmonization dictionary: JHU -> OWID country names\n",
    "name_map = {\n",
    "    \"US\": \"United States\",\n",
    "    \"Korea, South\": \"South Korea\",\n",
    "    \"Taiwan*\": \"Taiwan\",\n",
    "    \"Burma\": \"Myanmar\",\n",
    "    \"Congo (Kinshasa)\": \"Democratic Republic of Congo\",\n",
    "    \"Congo (Brazzaville)\": \"Republic of the Congo\",\n",
    "    \"Cote d'Ivoire\": \"Côte d'Ivoire\",\n",
    "    \"Holy See\": \"Vatican\",\n",
    "    \"Cape Verde\": \"Cabo Verde\",  # sometimes appears as 'Cabo Verde' in OWID\n",
    "    \"Eswatini\": \"Eswatini\",      # consistent\n",
    "    \"West Bank and Gaza\": \"Palestine\",\n",
    "    \"Bahamas\": \"The Bahamas\",\n",
    "    \"Gambia\": \"The Gambia\",\n",
    "    \"Timor-Leste\": \"East Timor\",\n",
    "    \"North Macedonia\": \"North Macedonia\",\n",
    "    \"Laos\": \"Laos\",\n",
    "    \"Sao Tome and Principe\": \"Sao Tome and Principe\",\n",
    "}\n",
    "\n",
    "country_daily_merge = country_daily.copy()\n",
    "country_daily_merge[\"country_std\"] = country_daily_merge[\"Country/Region\"].replace(name_map)\n",
    "\n",
    "# Some rows in JHU are not countries (e.g., cruise ships, overseas territories).\n",
    "# These may not have a continent — that's expected.\n",
    "merge_df = country_daily_merge.merge(\n",
    "    continents.rename(columns={\"country\": \"country_std\"}),\n",
    "    on=\"country_std\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "coverage = merge_df[[\"Country/Region\", \"country_std\", \"continent\"]].drop_duplicates()\n",
    "missing = coverage[\"continent\"].isna().sum()\n",
    "total = coverage.shape[0]\n",
    "print(f\"Merged country→continent coverage: {total - missing}/{total} matched, {missing} missing\")\n",
    "\n",
    "# Show a few missing examples\n",
    "missing_names = coverage[coverage[\"continent\"].isna()].head(20)\n",
    "display(missing_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e45c1",
   "metadata": {},
   "source": [
    "### Total deaths per continent (latest date)\n",
    "We compute totals using the latest date available and make a bar plot. Saved as `plots/continent_bar_plot.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_date = merge_df[\"date\"].max()\n",
    "continent_latest = (\n",
    "    merge_df[merge_df[\"date\"] == latest_date]\n",
    "    .dropna(subset=[\"continent\"])\n",
    "    .groupby(\"continent\", as_index=False)[\"total_deaths\"]\n",
    "    .sum()\n",
    "    .sort_values(\"total_deaths\", ascending=False)\n",
    ")\n",
    "\n",
    "display(continent_latest)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(continent_latest[\"continent\"], continent_latest[\"total_deaths\"])\n",
    "plt.title(f\"Total COVID-19 Deaths by Continent (as of {latest_date.date()})\")\n",
    "plt.xlabel(\"Continent\")\n",
    "plt.ylabel(\"Total deaths (cumulative)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = PLOTS_DIR / \"continent_bar_plot.png\"\n",
    "plt.savefig(out_path)\n",
    "plt.show()\n",
    "print(f\"Saved: {out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ecb72",
   "metadata": {},
   "source": [
    "### OPTIONAL: Stacked bar – total deaths per year by continent\n",
    "For each year, we take the **last available date in that year** and compute the cumulative totals per continent. We then plot a stacked bar chart and save it as `plots/continent_stacked_plot.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c730703",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df[\"year\"] = merge_df[\"date\"].dt.year\n",
    "\n",
    "# For each year, get last date present in that year\n",
    "last_dates_per_year = merge_df.groupby(\"year\")[\"date\"].max().rename(\"last_date\").reset_index()\n",
    "\n",
    "year_end = merge_df.merge(last_dates_per_year, on=\"year\")\n",
    "year_end = year_end[year_end[\"date\"] == year_end[\"last_date\"]]\n",
    "\n",
    "by_continent_year = (\n",
    "    year_end.dropna(subset=[\"continent\"])\n",
    "    .groupby([\"year\", \"continent\"], as_index=False)[\"total_deaths\"]\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "pivot = by_continent_year.pivot(index=\"year\", columns=\"continent\", values=\"total_deaths\").fillna(0).sort_index()\n",
    "\n",
    "ax = pivot.plot(kind=\"bar\", stacked=True, figsize=(9, 6), legend=True)\n",
    "ax.set_title(\"Total COVID-19 Deaths per Year by Continent (stacked)\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Total deaths (cumulative)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = PLOTS_DIR / \"continent_stacked_plot.png\"\n",
    "plt.savefig(out_path)\n",
    "plt.show()\n",
    "print(f\"Saved: {out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87139cc6",
   "metadata": {},
   "source": [
    "## Notes / Limitations\n",
    "- The JHU dataset contains a few non-country entities (e.g., *Diamond Princess*). Those typically do not map to continents and are left unmatched.\n",
    "- Some countries were renamed or have alternative spellings across sources; a small harmonization dictionary is included. You may expand it if needed to maximize merge coverage.\n",
    "- Daily new deaths can show spikes due to backfills or corrections. We clip negative values to zero to simplify plotting.\n",
    "- The notebook attempts a web fallback if the local `data/` files are missing. If your environment has no internet access, make sure to place both CSV files in `data/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a45bf",
   "metadata": {},
   "source": [
    "## Generative AI Acknowledgment\n",
    "Parts of this notebook (structure, comments, and helper functions) were created with assistance from ChatGPT (GPT‑5 Thinking). I validated the steps, ran the code, and ensured it aligns with course requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
